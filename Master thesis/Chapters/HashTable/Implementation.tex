
\section{Implementation}

We decided to implement different hash tables\index{Hash table}, in order to see, in practice, which models offered the best characteristics for our specific problem on graphic card. This also makes it possible to compare theoretical performance with that obtained in practice.
All implementations will aim to offer the same properties with the same uniqueness of elements and the same guarantees on reading/writing to be more fair; if there are simultaneous accesses (read and write) for the same key, the behaviour is undefined. We will mainly be interested in search and insertions operations per thread and not on a warp basis.

\subsection{Open addressing\index{Open addressing}}

These hash tables\index{Hash table} are very simple to implement. We start by filling them with a sentinel value representing the absence of value. When an element is inserted, based on the index obtained by the hash function and its relative offset depending on the strategy used, it is replaced atomically\index{Atomic} by the new key if the current value is still empty. However, if insertions were limited to this treatment, the results would not be as expected. Indeed, we would be able to read values that have not yet been written, which would lead to undefined behavior. We therefore chose to work with a second sentinel value that represents a key/value being inserted. The element will be accessible after the key has been changed by its final value. The tables are static, all the memory needed to hold the keys has already been pre-allocated, so you won't have to pay extra to transfer the elements from the old table to the new one in a amortized way.

\subsection{Chaining\index{Chaining}}

For the chaining\index{Chaining} policy, we have opted for a modified model where the elements are not stored in a simple linked-list but grouped by packet through a linked-list, like the idea that can be found in data structures of type deque. When several threads attempt to access this list, they attempt to atomically\index{Atomic} increment the number of items stored in the packet to determine where they should place their result. If there is not enough space left in the packet, a new node is allocated and they restart their operation to this new node. The number of buckets was determined as being twice the number of elements to insert divided by the number of elements in a linked-list node that fits on a single cache line\index{Block}.

\subsection{Cuckoo\index{Cuckoo hashing}}

Cuckoo hashing is more complicated despite a rather simple general concept, we insert an element at a certain position and if the position is already occupied, we expel it in order to replace it by our new element. The old one is then inserted into another table where the same process is repeated until the system stabilizes. The main advantage of this table is that the number of possible positions where an element can be located is limited by the number of tables used by this technique unlike open addressing\index{Open addressing} or chaining\index{Chaining}, which can lead to degenerate cases and linear complexities. Tables of the same size have been used to simplify the design but there are variants where the tables are not all of the same size.

In our case, we have opted for two different strategies to address some of the problems inherent in this hash table\index{Hash table}. Indeed, it is very likely that we fall into a cycle where we seek to insert in a loop the same subset of elements. It is then necessary to rebuild a new table by rehashing all the elements. To limit the occurrence of such problems and to avoid this significant cost, there are several solutions: the simplest is to use more than two tables, usually three or four. Each with its own hash function and we alternate cyclically between them when inserting. The other consists in creating something close to the chaining\index{Chaining}, one cell does not store only one element at a position but several of them and when there is no more place, one is expelled, generally the last one to work as a queue. Let's also mention the existence of ``stash'', a small area where we store all the keys that could not be successfully inserted~\cite{kirsch2009more} and its adaptation to GPU\index{Graphics cards}~\cite{khorasani2015stadium}.

\subsection{Remark}

All these implementations are intended to be lock-free\index{Lock-free}, unfortunately, if we insert this sentinel value representing an intermediate state synonymous with a current addition of an element, we have to wait until this operation is finished before we can continue our algorithm. This wait results in a primitive called \textit{spinlock}, an active wait on the value until a new value is set, which can greatly increase the contention and therefore the access time to the elements. Care must also be taken to pay close attention to the spinlock model used\index{SIMT}. Indeed, since threads are executed by warp, it is necessary to make sure that the processing making the situation evolve is carried out before the active wait. It is therefore necessary to play on the way the operations are scheduled in order to obtain the desired behavior and not a brutal dead lock by implementing the standard schema.
