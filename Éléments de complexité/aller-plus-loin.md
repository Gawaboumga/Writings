Dans cet article, nous n'avons fait que survoler des concepts de base liés à la notion de complexité et avons montré, sur un cas concret, les différents aspects qui pouvaient intervenir. Tout ceci n'était qu'une petite introduction par rapport à la multitude de questions qu'on peut se poser en s'intéressant davantage à la nature des choses. 

Nous avons abordé, ici, la notion de complexité, mais celle-ci englobe d'autres sous-catégories. Que ce soit en fonction de la situation initiale en entrée, on peut s'intéresser au cas moyen (une distribution aléatoire en entrée), au meilleur (celui qui nécessite le moins d'étapes) ou au pire cas (celui qui en nécessite le plus). Si la liste est déjà triée, il y aura sans doute moins de travail à effectuer que dans une configuration aléatoire ou très particulière par rapport à l'algorithme (imaginons qu'elle soit triée dans le sens opposé). Comment faut-il alors définir le cas moyen ? Peut-on toujours déterminer les motifs liés au pire ou meilleur cas ?

Il existe également une question opposée, quels sont tous les algorithmes qui ont pour complexité $\Omega(N \log N)$ ? Quid des autres, pourquoi ces différences ? Des tentatives de réponses sont données par la définition de classes de complexité, ensemble de problèmes dont la complexité est "équivalente". Ce sont des questions particulièrement ardue, la raison ontologique est rarement connue et on est, parfois, capable d'expliquer que des familles de problèmes ne pourront jamais avoir la même complexité que d'autres, mais de nombreux problèmes sont ouverts dans ce domaine depuis plus d'un cinquantenaire ...

Les modèles de calcul sont fort abstraits et associent une complexité unitaire à des opérations qui sont pourtant intuitivement différentes, additionner deux nombres semble plus simple que les diviser.

- On peut alors chercher à avoir des modèles de calcul plus précis, qui s'intéressent à la complexité intrinsèque de ces opérations de bases en étudiant la complexité par rapport au nombres de bits ou le nombre de portes logiques nécessaires [^VOLLMER, Heribert. Introduction to circuit complexity: a uniform approach. Springer Science & Business Media, 2013.]; cela a d'ailleurs donné naissance à des algorithmes de tri basé sur des réseaux, où on construit un circuit fixe dans lequel les données vont passer et ressortir magiquement triées [^PATERSON, Michael S. Improved sorting networks withO (logN) depth. Algorithmica, 1990, vol. 5, no 1-4, p. 75-92.].
- L'ordinateur sur lequel on va exécuter l'algorithme aura peut-être plus de faciliter avec certaines opérations par rapport à d'autres, et il sera parfois préférable d'employer des algorithmes ayant des complexités asymptotiques plus élevées mais qui possèdent des performances meilleurs pour les tailles de données qui nous intéresse parce que les facteurs multiplications d'un algorithme "théoriquement" meilleur le rendent en réalité moins intéressant.
- Plus fort encore, on peut démontrer que certaines propriétés sont dites *oblivious*, c'est à dire que cet algorithme est optimal peut importe les conditions dans lequel il est appliqué. On sait qu'il ne sera pas possible de faire mieux en terme de complexité pour la propriété qui nous intéresse dans le cadre général. C'est un saint Graal en informatique qui a déjà été atteint de nombreuses fois.

Nous avons laissé envisager que la complexité d'un problème était lié à la taille en entrée. Mais ce n'est qu'une vision partielle, il existe des problèmes dont la complexité peut dépendre de la taille du résultat en sortie. Par exemple: si je divise deux nombres (en les soustrayant), plus le résultat est petit, moins j'ai du faire de soustractions. Ou encore, il existe des algorithmes qui dépendent à la fois de l'entrée et de la sortie [^CHAN, Timothy M. Optimal output-sensitive convex hull algorithms in two and three dimensions. Discrete & Computational Geometry, 1996, vol. 16, no 4, p. 361-368.].

À l'heure actuelle, il est de plus en plus rare de considérer un algorithme sans ses possibilités de parallélisme, donc d'avoir plusieurs machines qui collaborent pour produire le résultat d'un problème. Cela entraîne de nouveaux problèmes théoriques pour savoir s'il est possible d'avoir des algorithmes équivalents à ceux séquentiels alors qu'il faut faire communiquer de l'information entre ces machines. Parcourir un graphe en parallèle est quelque peu plus ardu qu'en séquentiel. Quelles sont les conséquences du parallélisme sur la quantité d'information à échanger ou l'espace additionnel en mémoire qu'il faut alors gérer. On a également vu apparaître plus récemment des problématiques liées au réseau et à sa communication où il faut faire intervenir le nombre de machines qui discutent ainsi que le temps mis pour transférer l'information entre-elles[^DEAN, Jeffrey et GHEMAWAT, Sanjay. MapReduce: simplified data processing on large clusters. Communications of the ACM, 2008, vol. 51, no 1, p. 107-113.].

Nous n'avons également pas fait part de la possibilité d'employer de l'aléatoire dans notre algorithme. Cela peut paraître surprenant mais il est parfois intéressant de proposer une solution aléatoire et d'arriver petite à une solution correcte, on parle d'algorithme aléatoire (*randomized*). Encore plus étonnant, on peut consentir à avoir une marge d'erreur dans les résultats parce que cette complexité, qui dépend du nombre d'erreurs qu'on tolère, peut être largement plus intéressante qu'un algorithme exact, on les qualifie de probabiliste (*probabilistic*).

En algorithmique, l'autre point capital à tout programme est l'usage des structures de données (Algorithms + Data Structures = Programs comme disait Wirth[^WIRTH, Niklaus. Algorithms and data structures. 1986.]). Il y aurait une quantité astronomique de choses à dire sur celles-ci tant elles peuvent être liée intimement à certains algorithmes. Au delà de ça, on peut se demander quelles sont les complexités de certaines opérations (pour insérer un élément, pour trouver son prédécesseur, ...), ces questions sont complexes parce qu'il s'agit d'un doux mélange entre la complexité attendue par chaque opération et le nombre d'opérations qu'il est possible d'effectuer sur celles-ci. On peut se permettre d'éventuellement de ne jamais devoir supprimer un élément mais il faut que la recherche du prédécesseur soit absolument en O(1) par exemple. C'est généralement la contrainte de certaines opérations plus rapides au regard d'autres qui fait émerger ces nouvelles structures de données.

Un concept qui est intimement liée à ces structures de données est la complexité par rapport à non pas une seule opération, mais par rapport à une suite d'opérations (ce qui est plus représentatif de l'usage qu'on en fait en pratique). Cela peut se transcrire par des concepts de *stratégie* sur les opérations internes à la structure de données, si on attend d'avoir un nombre suffisant d'insertions ou de suppressions avant d'effectuer les changements adéquats par exemple. Et cela a aboutit à la complexité dite OPT, parfois notée $O_{OPT}$, définie comme suit: étant donnée une séquences d'opérations connues à l'avance (ou non) et généralement notée $\sigma$, il s'agit de la complexité optimale qui pourrait être atteinte. Il s'agit d'un concept extrêmement fort en algorithmique et dont certains résultats sont connus comme étant à un facteur multiplicatif près, on qualifie parfois de complexité *compétitive* si $ALG(\sigma) \leq c * OPT(\sigma)$.

On a parlé de mesurer le nombre d'opérations nécessaires pour arriver à une solution ou du réseau. Mais on ne s'est pas intéressé à l'espace mémoire nécessaire pour répondre à la question et ses éventuels liens avec la complexité temporelle que cela peut entraîner. Un autre modèle de calcul, apparu plus récemment, s'intéresse au dénombrement des transferts de données entre une mémoire infinie et une plus petite mémoire, l'idée étant qu'accéder à une donnée située à l'autre bout de l'internet est plus lent que de faire une simple addition. A l'air du réchauffement climatique, des modèles ont également été développés afin d'étudier l'énergie dépensée par des algorithmes, ce qui possède plusieurs applications dans le domaine de la sécurité. Il y a également toutes des panoplies liées à des machines spéciales, basées sur des formules mathématiques, sur des modèles d'ordinateur quantique, sur des machines plus puissantes que celle de Turing, ...