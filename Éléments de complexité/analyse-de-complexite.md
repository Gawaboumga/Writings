Une fois que les concepts sont définis, on peut commencer à s'amuser et s'intéresser à différentes problématiques.

La première étape serait de chercher un algorithme, que ce soit en posant un nouveau problème ou en répondant à une question déjà existante. Seulement tous les problèmes ne sont pas forcément intéressants, on cherche, par exemple: des algorithmes représentatifs d'une famille de tâches (trouver un élément dans une collection par exemple), ayant des relations avec d'autres domaines des Sciences (sur des graphes, sur de la logique, sur des systèmes d'équations, ...) ou dont la résolution fait preuve d'innovation (un nouvel outil, poser le problème autrement ou un résultat contre-intuitif).

Une fois qu'on a découvert un algorithme, on peut l'analyser pour exprimer sa complexité. Que ce soit pour obtenir une expression précise (afin de mieux évaluer des algorithmes analogues) ou par rapport à différents modèles de calcul (on peut s'intéresser à un aspect temporel sur le nombre d'étapes que prend l'algorithme ou à l'aspect spatial en considérant l'espace mémoire nécessaire pour y répondre). Encore faut-il être capable de pouvoir calculer la complexité du problème, cela peut s'avérer très compliqué, la conjecture de Collatz (ouverte depuis presque un siècle) est un bel exemple.

Une fois qu'on possède un algorithme dont on connait la complexité, est-ce qu'on est capable d'être plus efficace, de mieux employer les ressources ? C'est la question associée au $O(.)$. Par essence, si un algorithme est en $O(n)$, il est également en $O(n^{2})$, $O(n^{3})$, ... On cherche la fonction la "plus simple" et la "plus petite" qui borne supérieurement l'algorithme; la définition de "plus simple" est très arbitraire, c'est le même problème que pour définir des fractions "simples", on cherche une fonction exprimée en un nombre minimal de fonctions élémentaires, généralement: les polynômes, le logarithme ou l'exponentielle. Il se fait que les outils mathématiques ont tendance à exprimer les complexités en faisant intervenir ces fonctions-là, il est exceptionnel de voir des fonctions autres telles que celles trigonométriques par exemple.

Mais jusque où on peut aller en efficacité, est-ce qu'on peut toujours faire mieux ? Quelle est la complexité inhérente à un problème, on se dit bien que trier une collection est plus compliqué que de chercher un élément et qu'il ne sera jamais possible que les deux soient aussi simples. C'est la question associée au $\Omega(.)$. On retrouve les mêmes contraintes que pour la question précédente, sauf qu'on cherche la "plus simple" et la "plus grande" qui borne inférieurement l'algorithme. Cette question est d'autant plus critique lorsque l'on s'intéresse à la notion de "classe de complexité" qui regroupent les problèmes ayant une complexité minimale équivalente.