Dans cette (brève) introduction à l'I/O complexité, nous allons étudier des algorithmes dans un modèle de calcul différent. Souvent, durant les études, un modèle temporel de type RAM est employé afin de déterminer la complexité d'un algorithme. Il est en effet très naturel et propose une analyse proche de la conception mentale du problème. Seulement, il ne laisse pas transparaître certains phénomènes.

Dans cet article, nous retracerons rapidement les idées qui ont permis l'émergence de cette famille de modèles de calcul qui se consacre non pas à l'analyse temporelle des algorithmes (leur nombre d'opérations) mais bel et bien au nombre d'accès nécessaires afin de résoudre le dit problème et qualifiés d'I/O puisqu'on quantifie le nombre d'entrées/sorties des données. Nous commencerons par revenir brièvement sur des modèles qui ont présenté des tournants majeurs dans ce domaine par le biais d'un bref historique. Nous nous concentrerons alors sur un modèle bien précis : *External Memory* (ou « EM ») et verrons différents résultats obtenus dans celui-ci, avec quelques démonstrations. Enfin, nous terminerons sur une variante de ce modèle qui présente un aspect fort novateur et puissant : la notion d'« *obliviousness* ».

Ce texte requiert certaines bases en algorithmie, mais reste relativement abordable pour des étudiants du supérieur (master ou bachelier enthousiaste). Connaître a priori les notions de complexité classiques ([notation grand O](https://fr.wikipedia.org/wiki/Comparaison_asymptotique)) est une condition nécessaire ainsi que le fonctionnement de certains algorithmes classiques. La connaissance du [*Master Theorem*](https://fr.wikipedia.org/wiki/Master_theorem) et des bases en [théorie de l'information](https://fr.wikipedia.org/wiki/Th%C3%A9orie_de_l%27information) est un plus non négligeable.